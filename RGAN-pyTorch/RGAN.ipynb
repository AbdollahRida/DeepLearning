{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent GAN:\n",
    "________\n",
    "\n",
    "In this notebook we present a recurrent GAN (RGAN) that uses stacked LSTMs for both the generator and the discriminator. It is built to generate MNIST numbers seen as time-series. \n",
    "\n",
    "Before delving into the actual code, let's take a look at the math behind the model.\n",
    "\n",
    "## MNIST as time series:\n",
    "\n",
    "The MNIST hand-written digit dataset is ubiquitous in machine learning research. Accuracy is high enough to consider the problem *solved*, and generating MNIST digits isn't an issue for traditional GANs. However, sequential generation is less commonly done. To serialize the images, each $28 \\times 28$ image is flattened into 784-dimensional vector, which is a sequence we aim to generate with the RGAN.\n",
    "\n",
    "## Recurrent GAN model:\n",
    "\n",
    "### Discriminator:\n",
    "\n",
    "The discriminator is trained to minimize the average negative cross-entropy between its predictions *per time-step* and the labels of the sequence. If we denote by $RNN(X)$ the vector of outputs from an RNN receiving $X$ as input, then the loss is:\n",
    "\n",
    "$$D_{loss}(X_n, y_n) = - CE(RNN_D(X_n), y_n)$$\n",
    "\n",
    "For real sequences $y_n$ is a vector of 1s, or 0s for synthetic sequences. \n",
    "\n",
    "### Generator:\n",
    "\n",
    "The objective for the generator is then to trick the discriminator into classifying its outputs as true. It thus wishes to minimize the average negative cross-entropy between the discriminator's predictions on generated synthetic sequences and the *true* label, the vector of 1s (written $\\mathbb{1}$).\n",
    "\n",
    "$$G_{loss}(Z_n) = D_{loss}(RNN_G(Z_n), \\mathbb{1})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "#from skimage.io import imsave\n",
    "import os\n",
    "#from tensorboardX import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "def gpu(tensor, gpu=use_gpu):\n",
    "    if gpu:\n",
    "        return tensor.cuda()\n",
    "    else:\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 28\n",
    "img_width = 28\n",
    "img_size = img_height * img_width\n",
    "\n",
    "to_train = True\n",
    "to_restore = False\n",
    "output_path = \"output\"\n",
    "\n",
    "max_epoch = 1000\n",
    "\n",
    "hg_size = 150\n",
    "hd_size = 300\n",
    "z_size = 100\n",
    "batch_size = 256\n",
    "seq_size=4\n",
    "n_hidden=300\n",
    "tr_data_num=60000;\n",
    "g_num_layers=2;\n",
    "d_num_layers=2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/majrda/Scripts/data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, stddev = 0.1):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, din):\n",
    "        if self.training:\n",
    "            return din + torch.autograd.Variable(torch.randn(din.size()).cuda() * self.stddev)\n",
    "        return din"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Generator, self).__init__()\n",
    "\n",
    "    self.lstm_G = nn.LSTM(input_size = z_size, \n",
    "                           hidden_size = n_hidden,\n",
    "                           num_layers = g_num_layers,\n",
    "                           bias = True)\n",
    "    \n",
    "    self.Lrelu = nn.LeakyReLU()\n",
    "    \n",
    "    self.MLP = nn.Linear(n_hidden, img_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.unsqueeze(1)\n",
    "    #print(x.size())\n",
    "    output, _ = self.lstm_G(x)\n",
    "    output = torch.tanh(self.MLP(self.Lrelu(output)))\n",
    "    #print(output[0,:,0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Discriminator, self).__init__()\n",
    "    \n",
    "    self.lstm_D = nn.LSTM(input_size = img_size, \n",
    "                           hidden_size = n_hidden,\n",
    "                           num_layers = d_num_layers,\n",
    "                           bias = True)\n",
    "    \n",
    "    self.Lrelu = nn.LeakyReLU()\n",
    "    \n",
    "    self.MLP = nn.Linear(n_hidden, 1)\n",
    "    \n",
    "    self.noise = GaussianNoise(.3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    outputs, _ = self.lstm_D(x)\n",
    "    #print(outputs.size())\n",
    "    outputs = self.noise(outputs)\n",
    "    res = self.MLP(self.Lrelu(outputs[:, -1, :]))\n",
    "    #print(res)\n",
    "    y_data = torch.sigmoid(res.narrow(0, 0, x[0].shape[0]))\n",
    "    return y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = datasets.MNIST(root=root_dir, train=True, download=False, transform=transforms.ToTensor())\n",
    "mnist_testset = datasets.MNIST(root=root_dir, train=False, download=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=mnist_trainset,\n",
    "                                       batch_size=batch_size, \n",
    "                                       shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=mnist_testset,\n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "nb_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D_epoch = []\n",
    "loss_G_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net_G = gpu(Generator())\n",
    "net_D = gpu(Discriminator())\n",
    "\n",
    "optimizer_G = torch.optim.Adam(net_G.parameters(),lr=lr)\n",
    "optimizer_D = torch.optim.Adam(net_D.parameters(),lr=lr)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for e in range(nb_epochs):\n",
    "    print(\"Epoch \",e)\n",
    "    loss_G = 0\n",
    "    loss_D = 0\n",
    "    for t, real_batch in enumerate(tqdm(train_loader)):\n",
    "        \n",
    "        #improving D\n",
    "        z = gpu(torch.empty(batch_size,z_size).normal_())\n",
    "        fake_batch = net_G(z)\n",
    "        #print(fake_batch.shape)\n",
    "        #print(fake_batch.shape)\n",
    "        \n",
    "        D_scores_on_fake = net_D(fake_batch)\n",
    "        #print(D_scores_on_fake)\n",
    "        #print(real_batch[0])\n",
    "        #print(real_batch[0].view(256, 1, 784))\n",
    "        D_scores_on_real = net_D(real_batch[0].view(real_batch[0].shape[0], 1, 784).cuda())\n",
    "        #print(D_scores_on_real)\n",
    "            \n",
    "        loss = -torch.mean(torch.log(1-D_scores_on_fake[0]) + torch.log(D_scores_on_real[0]))\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_D.step()\n",
    "        loss_D += loss\n",
    "                    \n",
    "        # improving G\n",
    "        z = gpu(torch.empty(batch_size,z_size).normal_())\n",
    "        fake_batch = net_G(z)\n",
    "        D_scores_on_fake = net_D(fake_batch)\n",
    "            \n",
    "        loss = -torch.mean(torch.log(D_scores_on_fake[0]))\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_G.step()\n",
    "        loss_G += loss\n",
    "           \n",
    "    loss_D_epoch.append(loss_D)\n",
    "    loss_G_epoch.append(loss_G)\n",
    "    print(\"Loss on Generator this epoch: {}\\nLoss on Discriminator this epoch: {}\".format(loss_G, loss_D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(loss_D_epoch)\n",
    "plt.plot(loss_G_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = gpu(torch.empty(batch_size,z_size).normal_())\n",
    "fake_samples = net_G(z)\n",
    "fake_data = fake_samples.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fake_data[0, 0]\n",
    "\n",
    "x = x.reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(x, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
